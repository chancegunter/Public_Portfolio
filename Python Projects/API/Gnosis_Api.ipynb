{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from datetime import date, timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Adding the parent directory of the current file to the system path\n",
    "cur_dir = Path(__file__).parent\n",
    "sys.path.append(str(cur_dir / '../../../../../../Credentials'))\n",
    "\n",
    "# Importing credentials module\n",
    "import credentials\n",
    "\n",
    "# API URL\n",
    "url = \"https://api.freight.fyi/api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_read():\n",
    "    # Connect to the database using the provided credentials\n",
    "    conn = mysql.connector.connect(host=credentials.host_read,\n",
    "                                   database=credentials.database_hq,\n",
    "                                   user=credentials.username,\n",
    "                                   password=credentials.password,\n",
    "                                   ssl_disabled=True)\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    # API endpoint for authentication\n",
    "    authurl = url + 'auth/token'\n",
    "\n",
    "    # Payload containing username and password\n",
    "    payload = {\n",
    "        'username': credentials.gnosis_username,\n",
    "        'password': credentials.gnosis_password\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "\n",
    "    # Send a POST request to authenticate and obtain the access token\n",
    "    response = requests.request(\"POST\", authurl, headers=headers, data=payload)\n",
    "    auth_tkn = response.json()['access_token']\n",
    "\n",
    "    # Print the response JSON containing the access token\n",
    "    print(response.json())\n",
    "\n",
    "    return auth_tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_data(conn_read, original_path):\n",
    "    # Change the current working directory to the original path\n",
    "    os.chdir(original_path)\n",
    "\n",
    "    # Move up one level and navigate to the 'Queries' folder\n",
    "    os.chdir('../Queries')\n",
    "    folder = os.path.abspath(os.curdir)\n",
    "\n",
    "    # Open the SQL query file and read its contents\n",
    "    with open(folder + '/POST_Update_query.sql', 'r') as data_query:\n",
    "        # Execute the SQL query and store the result in a DataFrame\n",
    "        container_df = pd.read_sql_query(data_query.read(), conn_read)\n",
    "\n",
    "    # Return the DataFrame containing the query result\n",
    "    return container_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postings(container_df, auth_tkn):\n",
    "    try:\n",
    "        url = f'https://api.freight.fyi/api/v1/tracking_requests/'\n",
    "        url_scac = url + 'with_carrier'\n",
    "\n",
    "        headers = {\n",
    "            'accept': 'application/json',\n",
    "            'Authorization': f'Bearer {auth_tkn}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        payload = []\n",
    "\n",
    "        # Filter containers with carrier SCAC code\n",
    "        container_scac_df = container_df[(~container_df['opo_container_carrier'].isnull())].drop_duplicates()\n",
    "        # Filter containers without carrier SCAC code\n",
    "        container_noscac_df = container_df[(container_df['opo_container_carrier'].isnull())].drop_duplicates()\n",
    "\n",
    "        # Convert container_scac_df to a list of dictionaries\n",
    "        scac_list = container_scac_df.to_dict('records')\n",
    "\n",
    "        # Create payload for containers with carrier SCAC code\n",
    "        for container in scac_list:\n",
    "            container_dict = {\n",
    "                \"mbl_number\": f'{container[\"opo_container_hbl_number\"]}',\n",
    "                \"carrier_scac\": f'{container[\"opo_container_carrier\"]}'\n",
    "            }\n",
    "            payload.append(container_dict)\n",
    "\n",
    "        # Send API request for containers with carrier SCAC code\n",
    "        response = requests.post(url_scac, headers=headers, json=payload)\n",
    "        resp_scac = response.json()\n",
    "        answers = []\n",
    "\n",
    "        # Process API response for containers with carrier SCAC code\n",
    "        for d in resp_scac.keys():\n",
    "            resp_scac[d].update({\"mbl_number\": f\"{str(d)}\"})\n",
    "            answers.append(resp_scac[d])\n",
    "\n",
    "        # Create DataFrame for containers with carrier SCAC code\n",
    "        with_scac = pd.DataFrame.from_records(answers)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions and print error details\n",
    "        print('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "        print(e)\n",
    "        quit()\n",
    "\n",
    "    try:\n",
    "        url = f'https://api.freight.fyi/api/v1/tracking_requests/'\n",
    "\n",
    "        headers = {\n",
    "            'accept': 'application/json',\n",
    "            'Authorization': f'Bearer {auth_tkn}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        container_noscac_lst = container_noscac_df[\"opo_container_hbl_number\"].values.T.tolist()\n",
    "\n",
    "        payload = {\n",
    "            \"mbl_numbers\": container_noscac_lst\n",
    "        }\n",
    "\n",
    "        # Send API request for containers without carrier SCAC code\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        resp_scac = response.json()\n",
    "        answers = []\n",
    "\n",
    "        # Process API response for containers without carrier SCAC code\n",
    "        for d in resp_scac.keys():\n",
    "            resp_scac[d].update({\"mbl_number\": f\"{str(d)}\"})\n",
    "            answers.append(resp_scac[d])\n",
    "\n",
    "        # Create DataFrame for containers without carrier SCAC code\n",
    "        without_scac = pd.DataFrame.from_records(answers)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions and print error details\n",
    "        print('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "        print(e)\n",
    "        quit()\n",
    "\n",
    "    return with_scac, without_scac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containers(auth_tkn):\n",
    "    act_lst = []  # List to store active containers\n",
    "    header = []  # List to store header information\n",
    "    i = 0  # Counter for pagination\n",
    "    worth = True  # Flag to control the loop\n",
    "\n",
    "    while worth:\n",
    "        try:\n",
    "            url = f'https://api.freight.fyi/api/v1/containers/active/?page_num={i}&page_size=50'\n",
    "\n",
    "            payload = {}\n",
    "            headers = {\n",
    "                'Authorization': f'Bearer {auth_tkn}',\n",
    "                'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            }\n",
    "\n",
    "            response = requests.request(\"GET\", url, headers=headers, data=payload, timeout=45)\n",
    "            shipments = response.json()\n",
    "\n",
    "            if len(header) == 0:\n",
    "                header = [key for key in response.json()['containers'][0]]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            for cargo in shipments['containers']:\n",
    "                act_lst.append(cargo)\n",
    "\n",
    "            print(shipments['metadata'])\n",
    "\n",
    "            if i == shipments['metadata']['last_page']:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "    container_lst = []  # List to store all containers\n",
    "    i = 0  # Counter for pagination\n",
    "    worth = True  # Flag to control the loop\n",
    "\n",
    "    while worth:\n",
    "        try:\n",
    "            url = f'https://api.freight.fyi/api/v1/containers/?page_num={i}&page_size=50'\n",
    "\n",
    "            payload = {}\n",
    "            headers = {\n",
    "                'Authorization': f'Bearer {auth_tkn}',\n",
    "                'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            }\n",
    "\n",
    "            response = requests.request(\"GET\", url, headers=headers, data=payload, timeout=45)\n",
    "            shipments = response.json()\n",
    "\n",
    "            if len(header) == 0:\n",
    "                header = [key for key in response.json()['containers'][0]]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            for cargo in shipments['containers']:\n",
    "                container_lst.append(cargo)\n",
    "\n",
    "            print(shipments['metadata'])\n",
    "\n",
    "            if i == shipments['metadata']['last_page']:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "            break\n",
    "\n",
    "    downloads_path = str(Path.home() / \"Downloads\")\n",
    "    os.chdir(downloads_path)\n",
    "\n",
    "    container_df = pd.DataFrame.from_records(container_lst)\n",
    "    act_df = pd.DataFrame.from_records(act_lst)\n",
    "\n",
    "    return container_df, act_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(container_df, conn_read):\n",
    "    container_num = container_df[['container_number']].drop_duplicates()\n",
    "    container_lst = container_num.astype(str).values.T.tolist()\n",
    "    container_constraint = '\\'' + '\\', \\''.join(container_lst[0]) + '\\''\n",
    "\n",
    "    query = f\"\"\"\n",
    "SELECT  as container_number,\n",
    "        as mbl_number,\n",
    "        as org_scac,\n",
    "        as org_container_name,\n",
    "        as org_mother_vessel,\n",
    "        as org_mother_voyage,\n",
    "        as org_origin_port,\n",
    "        as org_pod_city,\n",
    "        as org_final_discharge,\n",
    "        as org_vessel_atd,\n",
    "        as org_discharged_dt,\n",
    "        as org_rail_departed_dt,\n",
    "        as org_ramp_eta,\n",
    "        as org_eta,\n",
    "        as org_demurrage_day,\n",
    "        as org_out_gate,\n",
    "        as org_detention_day,\n",
    "        as org_empty_returned,\n",
    "        as org_customs_clearance,\n",
    "        as org_carrier_release,\n",
    "        as org_status_id\n",
    "    \"\"\"\n",
    "\n",
    "    checked = pd.read_sql(query, conn_read)\n",
    "\n",
    "    return checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readin(original_path, today_date):\n",
    "#     os.chdir(original_path)\n",
    "#     folder = r'\\\\fileserver\\public\\Purchasing\\Logistics\\Gnosis\\API'\n",
    "#     os.chdir(folder)\n",
    "#     file_name = 'API_Results' + today_date.strftime(\"%d%m%Y\") + '.xlsx'\n",
    "#     file_name = '\\\\API_Results_2023_04_27.xlsx'\n",
    "#     file_name = '\\\\API_Results_2023_05_22.xlsx'\n",
    "\n",
    "#     sheet = [{'Name': 'POST_Scac', 'df': 'with_scac'},\n",
    "#              {'Name': 'POST', 'df': 'without_scac'},\n",
    "#              {'Name': 'Containers', 'df': 'container_df'},\n",
    "#              {'Name': 'Active', 'df': 'act_df'}]\n",
    "\n",
    "#     with_scac = pd.read_excel(folder + file_name, sheet_name=sheet[0]['Name'])\n",
    "#     without_scac = pd.read_excel(folder + file_name, sheet_name=sheet[1]['Name'])\n",
    "#     container_df = pd.read_excel(folder + file_name, sheet_name=sheet[2]['Name'])\n",
    "#     act_df = pd.read_excel(folder + file_name, sheet_name=sheet[3]['Name'])\n",
    "\n",
    "#     return with_scac, without_scac, container_df, act_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulations(container_df, checked_df, today_date):\n",
    "    container_df = container_df[['container_number', 'container_journey_start_key', 'ocean_carrier_scac',\n",
    "                                 'mother_vessel', 'mother_voyage', 'pol_city', 'pod_city', 'final_dest_city',\n",
    "                                 'loaded_on_vessel_dt', 'vessel_atd_dt', 'discharged_dt', 'rail_departed_dt',\n",
    "                                 'rail_ata_dt', 'rail_eta_dt', 'gnosis_rail_eta_dt', 'vessel_eta_dt',\n",
    "                                 'gnosis_estimated_discharge_dt', 'last_free_demurrage_day_dt',\n",
    "                                 'gnosis_estimated_last_free_demurrage_day_dt', 'last_free_detention_day_dt',\n",
    "                                 'gnosis_estimated_last_free_detention_day_dt', 'out_gate_dt', 'empty_returned_dt',\n",
    "                                 'customs_clearance_dt', 'carrier_release_dt']]\n",
    "\n",
    "    # replacing nan with non\n",
    "    container_df = container_df.replace({np.nan: None})\n",
    "\n",
    "    # Splitting out the MBL from the container number in Journey Key\n",
    "    container_df['container_journey_start_key'] = pd.Series(\n",
    "        container_df['container_journey_start_key']).str.split('_').str[1]\n",
    "\n",
    "    # Converting Data to Date\n",
    "    container_df['rail_eta_dt'] = pd.to_datetime(container_df['rail_eta_dt']).dt.date\n",
    "    container_df['vessel_eta_dt'] = pd.to_datetime(container_df['vessel_eta_dt']).dt.date\n",
    "    container_df['loaded_on_vessel_dt'] = pd.to_datetime(container_df['loaded_on_vessel_dt']).dt.date\n",
    "    container_df['vessel_atd_dt'] = pd.to_datetime(container_df['vessel_atd_dt']).dt.date\n",
    "    container_df['discharged_dt'] = pd.to_datetime(container_df['discharged_dt']).dt.date\n",
    "    container_df['rail_departed_dt'] = pd.to_datetime(container_df['rail_departed_dt']).dt.date\n",
    "    container_df['rail_ata_dt'] = pd.to_datetime(container_df['rail_ata_dt']).dt.date\n",
    "    container_df['gnosis_rail_eta_dt'] = pd.to_datetime(container_df['gnosis_rail_eta_dt']).dt.date\n",
    "    container_df['gnosis_estimated_discharge_dt'] = pd.to_datetime(\n",
    "        container_df['gnosis_estimated_discharge_dt']).dt.date\n",
    "    container_df['last_free_demurrage_day_dt'] = pd.to_datetime(container_df['last_free_demurrage_day_dt']).dt.date\n",
    "    container_df['gnosis_estimated_last_free_demurrage_day_dt'] = pd.to_datetime(\n",
    "        container_df['gnosis_estimated_last_free_demurrage_day_dt']).dt.date\n",
    "    container_df['last_free_detention_day_dt'] = pd.to_datetime(container_df['last_free_detention_day_dt']).dt.date\n",
    "    container_df['gnosis_estimated_last_free_detention_day_dt'] = pd.to_datetime(\n",
    "        container_df['gnosis_estimated_last_free_detention_day_dt']).dt.date\n",
    "    container_df['out_gate_dt'] = pd.to_datetime(container_df['out_gate_dt']).dt.date\n",
    "    container_df['empty_returned_dt'] = pd.to_datetime(container_df['empty_returned_dt']).dt.date\n",
    "    container_df['customs_clearance_dt'] = pd.to_datetime(container_df['customs_clearance_dt']).dt.date\n",
    "    container_df['carrier_release_dt'] = pd.to_datetime(container_df['carrier_release_dt']).dt.date\n",
    "\n",
    "    # Condition & Value logic for particular columns (vessel_atd, demurrage_day, detention_day, ramp_eta)\n",
    "\n",
    "    conditions = [\n",
    "        container_df['loaded_on_vessel_dt'].isna(),\n",
    "        container_df['loaded_on_vessel_dt'].notna()\n",
    "    ]\n",
    "    values = [\n",
    "        container_df['vessel_atd_dt'],\n",
    "        container_df['loaded_on_vessel_dt']\n",
    "    ]\n",
    "    container_df['vessel_atd'] = np.select(conditions, values)\n",
    "\n",
    "    conditions = [\n",
    "        container_df['last_free_demurrage_day_dt'].isna(),\n",
    "        container_df['last_free_demurrage_day_dt'].notna()\n",
    "    ]\n",
    "    values = [\n",
    "        container_df['gnosis_estimated_last_free_demurrage_day_dt'],\n",
    "        container_df['last_free_demurrage_day_dt']\n",
    "    ]\n",
    "    container_df['demurrage_day'] = np.select(conditions, values)\n",
    "\n",
    "    conditions = [\n",
    "        container_df['last_free_detention_day_dt'].isna(),\n",
    "        container_df['last_free_detention_day_dt'].notna()\n",
    "    ]\n",
    "    values = [\n",
    "        container_df['gnosis_estimated_last_free_detention_day_dt'],\n",
    "        container_df['last_free_detention_day_dt']\n",
    "    ]\n",
    "    container_df['detention_day'] = np.select(conditions, values)\n",
    "\n",
    "    conditions = [container_df['rail_ata_dt'].notna(),\n",
    "                  container_df['rail_ata_dt'].isnull() & container_df['rail_eta_dt'].notna(),\n",
    "                  container_df['discharged_dt'].notna(),\n",
    "                  container_df['vessel_eta_dt'].notna(),\n",
    "                  container_df['gnosis_estimated_discharge_dt'].notna(),\n",
    "                  container_df[['rail_ata_dt', 'rail_eta_dt', 'discharged_dt', 'vessel_eta_dt']].isna().all(axis=1)\n",
    "                  ]\n",
    "    values = [container_df['rail_ata_dt'],\n",
    "              container_df['rail_eta_dt'],\n",
    "              container_df['discharged_dt'],\n",
    "              container_df['vessel_eta_dt'],\n",
    "              container_df['gnosis_estimated_discharge_dt'],\n",
    "              pd.NaT  # default value for when all conditions are False\n",
    "              ]\n",
    "\n",
    "    container_df['ramp_eta'] = np.select(conditions, values)\n",
    "    container_df['eta'] = container_df['ramp_eta'] + timedelta(days=5)\n",
    "\n",
    "    container_df.rename(columns={\n",
    "        'container_journey_start_key': 'mbl_number',\n",
    "        'ocean_carrier_scac': 'gnosis_scac',\n",
    "        'mother_vessel': 'gnosis_mother_vessel',\n",
    "        'mother_voyage': 'gnosis_mother_voyage',\n",
    "        'pol_city': 'gnosis_origin_port',\n",
    "        'pod_city': 'gnosis_pod_city',\n",
    "        'final_dest_city': 'gnosis_final_discharge',\n",
    "        'vessel_atd': 'gnosis_vessel_atd',\n",
    "        'discharged_dt': 'gnosis_discharged_dt',\n",
    "        'rail_departed_dt': 'gnosis_rail_departed_dt',\n",
    "        'ramp_eta': 'gnosis_ramp_eta',\n",
    "        'eta': 'gnosis_eta',\n",
    "        'demurrage_day': 'gnosis_demurrage_day',\n",
    "        'out_gate_dt': 'gnosis_out_gate',\n",
    "        'detention_day': 'gnosis_detention_day',\n",
    "        'empty_returned_dt': 'gnosis_empty_returned',\n",
    "        'customs_clearance_dt': 'gnosis_customs_clearance',\n",
    "        'carrier_release_dt': 'gnosis_carrier_release'\n",
    "    }, inplace=True)\n",
    "\n",
    "    container_df = container_df[['container_number', 'mbl_number', 'gnosis_scac', 'gnosis_mother_vessel',\n",
    "                                 'gnosis_mother_voyage', 'gnosis_origin_port', 'gnosis_pod_city',\n",
    "                                 'gnosis_final_discharge', 'gnosis_vessel_atd', 'gnosis_discharged_dt',\n",
    "                                 'gnosis_rail_departed_dt', 'gnosis_ramp_eta', 'gnosis_eta', 'gnosis_demurrage_day',\n",
    "                                 'gnosis_out_gate', 'gnosis_detention_day', 'gnosis_empty_returned',\n",
    "                                 'gnosis_customs_clearance', 'gnosis_carrier_release']]\n",
    "\n",
    "    # Merging with HQ exception Data\n",
    "    upload_df = pd.merge(container_df,\n",
    "                         checked_df,\n",
    "                         on=['container_number', 'mbl_number'],\n",
    "                         how='left')\n",
    "\n",
    "    upload_df = upload_df.replace({np.nan: None})\n",
    "    upload_df['exception'] = None\n",
    "    upload_df['notes'] = None\n",
    "    upload_df['dropper'] = None\n",
    "\n",
    "    return upload_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_upload(upload_df, today_date):\n",
    "    complete_df = upload_df.copy()\n",
    "    date_cols = ['org_vessel_atd', 'org_discharged_dt', 'org_rail_departed_dt', 'org_ramp_eta', 'org_eta',\n",
    "                 'org_demurrage_day', 'org_out_gate', 'org_detention_day', 'org_empty_returned', 'org_customs_clearance',\n",
    "                 'org_carrier_release']\n",
    "\n",
    "    year_from = today_date + relativedelta(years=1)\n",
    "    year_ago = today_date - relativedelta(years=1)\n",
    "\n",
    "    # downloads_path = str(Path.home() / \"Downloads\")\n",
    "    # os.chdir(downloads_path)\n",
    "    # upload_df.to_csv('trail.csv')\n",
    "\n",
    "    for column in date_cols:\n",
    "        complete_df[column] = complete_df[column].apply(lambda val: None if val is None or pd.isna(val)\n",
    "                                                        else datetime.strptime(str(val), \"%Y-%m-%d\") if pd.notna(val)\n",
    "                                                        else None)\n",
    "        # Convert valid dates, handle None and NaN\n",
    "        complete_df[column] = pd.to_datetime(complete_df[column], errors='coerce')\n",
    "        # 'coerce' invalid dates to NaT\n",
    "        complete_df[column] = complete_df[column].dt.date\n",
    "\n",
    "    returned_df = complete_df[(complete_df['org_empty_returned'].notna())]\n",
    "    complete_df = complete_df[complete_df['org_empty_returned'].isna()]\n",
    "\n",
    "    org_list = [col for col in complete_df if col.startswith('org_')]\n",
    "    gnosis_list = [col for col in complete_df if col.startswith('gnosis_')]\n",
    "    gnosis_list.remove('gnosis_scac')\n",
    "\n",
    "    complete_df['exception'] = np.where(complete_df[org_list[1:]].isna().all(1), 1, 0)\n",
    "    complete_df['dropper'] = np.where(complete_df[gnosis_list[1:]].isna().all(1), 1, 0)\n",
    "    complete_df['dropper'] = np.where(complete_df[org_list[1:]].isna().all(1), 1, 0)\n",
    "    complete_df['dropper'] = np.where(complete_df[gnosis_list[1:]].isna().all(1), 2, complete_df['dropper'])\n",
    "    complete_df['count'] = complete_df.groupby('container_number')['container_number'].transform('size')\n",
    "\n",
    "    condition = [(complete_df['exception'] == 1) | (complete_df['dropper'] == 1)]\n",
    "    condition = [(complete_df['exception'] == 1) | (complete_df['dropper'] >= 1)]\n",
    "    value = [1]\n",
    "    complete_df['exception'] = np.select(condition, value)\n",
    "\n",
    "    cols = [\n",
    "        'scac', 'origin_port', 'mother_vessel', 'mother_voyage', 'pod_city', 'final_discharge',\n",
    "        'ramp_eta', 'eta', 'customs_clearance', 'carrier_release', 'vessel_atd',\n",
    "        'discharged_dt', 'rail_departed_dt', 'out_gate', 'empty_returned'\n",
    "    ]\n",
    "\n",
    "    excpt_cols = [\n",
    "        'demurrage_day', 'detention_day'\n",
    "    ]\n",
    "\n",
    "    date_fields = [\n",
    "        'demurrage_day', 'detention_day',\n",
    "    ]\n",
    "\n",
    "    def is_mismatch(x):\n",
    "        mismatch = []\n",
    "        for o in range(len(excpt_cols)):\n",
    "            gnosis_col = 'gnosis_' + excpt_cols[o]\n",
    "            org_col = 'org_' + excpt_cols[o]\n",
    "            if (excpt_cols[o] in date_fields) and (not pd.isnull(x[gnosis_col])):\n",
    "                if pd.isnull(x[org_col]) or (x[gnosis_col] < x[org_col]):\n",
    "                    mismatch.append(x[gnosis_col])\n",
    "                else:\n",
    "                    mismatch.append(None)\n",
    "            elif pd.isnull(x[gnosis_col]) and pd.isnull(x[org_col]):\n",
    "                mismatch.append(None)\n",
    "            elif x[gnosis_col] != x[org_col]:\n",
    "                mismatch.append('mismatch')\n",
    "            else:\n",
    "                mismatch.append(None)\n",
    "        return pd.Series(mismatch)\n",
    "\n",
    "    def is_match(x):\n",
    "        match = []\n",
    "        for e in range(len(cols)):\n",
    "            gnosis_col = 'gnosis_' + cols[e]\n",
    "            org_col = 'org_' + cols[e]\n",
    "            if pd.isnull(x[org_col]) or pd.isna(x[org_col]):\n",
    "                match.append(x[gnosis_col])\n",
    "            elif x[org_col] is not None and x[gnosis_col] is not None and x[org_col] != x[gnosis_col]:\n",
    "                match.append(x[gnosis_col])\n",
    "            else:\n",
    "                match.append(None)\n",
    "\n",
    "        return pd.Series(match)\n",
    "\n",
    "    complete_df[cols] = complete_df.apply(is_match, axis=1)\n",
    "    complete_df[excpt_cols] = complete_df.apply(is_mismatch, axis=1)\n",
    "\n",
    "    arr = np.where(complete_df.eq('mismatch'), complete_df.columns + ', ', '').sum(axis=1)\n",
    "    arr = list(arr)\n",
    "\n",
    "    complete_df['notes'] = arr\n",
    "    complete_df['notes'] = np.where(complete_df['notes'] == '', None, complete_df['notes'])\n",
    "    complete_df['exception'] = np.where(complete_df['notes'].notna(), 1, complete_df['exception'])\n",
    "\n",
    "    complete_df = complete_df[complete_df['dropper'] < 2]\n",
    "\n",
    "    exceptions = complete_df[(complete_df['exception'] == 1)]\n",
    "\n",
    "    complete_df.rename(columns={\n",
    "        'scac': 'Steamship Line',\n",
    "        'mother_vessel': 'Vessel',\n",
    "        'mother_voyage': 'Voyage',\n",
    "        'origin_port': 'Origin Port',\n",
    "        'pod_city': 'Discharge Port',\n",
    "        'final_discharge': 'Final Discharge',\n",
    "        'vessel_atd': 'Actual Ship Date',\n",
    "        'discharged_dt': 'Discharge Date',\n",
    "        'rail_departed_dt': 'Loaded On Rail',\n",
    "        'ramp_eta': 'ETA_Ramp',\n",
    "        'origin_port': 'Origin Port',\n",
    "        'pod_city': 'Discharge Port',\n",
    "        'final_discharge': 'Final Discharge',\n",
    "        'vessel_atd': 'Actual Ship Date',\n",
    "        'discharged_dt': 'Discharge Date',\n",
    "        'rail_departed_dt': 'Loaded on Rail',\n",
    "        'ramp_eta': 'ETA Ramp',\n",
    "        'eta': 'ETA',\n",
    "        'demurrage_day': 'Last Free Date Pull',\n",
    "        'out_gate': 'Date Pulled',\n",
    "        'detention_day': 'Last Free Date Return',\n",
    "        'empty_returned': 'Date Returned',\n",
    "        'customs_clearance': 'Customs Clearance Date',\n",
    "        'carrier_release': 'BL Release Date'\n",
    "    }, inplace=True)\n",
    "\n",
    "    complete_df = complete_df[[\n",
    "        'org_container_name', 'container_number', 'mbl_number',\n",
    "        'Steamship Line', 'Vessel', 'Voyage', 'Origin Port', 'Discharge Port',\n",
    "        'Final Discharge', 'Actual Ship Date', 'Discharge Date',\n",
    "        'Loaded on Rail', 'ETA Ramp', 'ETA', 'Last Free Date Pull',\n",
    "        'Last Free Date Return', 'Date Pulled', 'Date Returned',\n",
    "        'Customs Clearance Date', 'BL Release Date']]\n",
    "\n",
    "    complete_df.rename(columns={'org_container_name': 'PO # -or- Container Name'}, inplace=True)\n",
    "    complete_df.rename(columns={'container_number': 'Container Number'}, inplace=True)\n",
    "    complete_df.rename(columns={'mbl_number': 'HBL'}, inplace=True)\n",
    "    complete_df['ETD'] = None\n",
    "\n",
    "    complete_df = complete_df[complete_df['PO # -or- Container Name'].notna()]\n",
    "\n",
    "    complete_df['Carrier', 'Consignee', 'Manufacturer', 'Req. Ship Date',\n",
    "                'Cargo Ready Date', 'Status', 'Hot Container', 'Freight Cost',\n",
    "                'Customer Importer of Record', 'Door', 'Linked Orders',\n",
    "                'Container Notes', 'Scanner Notes', 'Receipt Date', 'Maersk DIR', 'External PO ID'] = None\n",
    "\n",
    "    complete_df = complete_df.reindex(columns=[\n",
    "        'Carrier', 'PO # -or- Container Name', 'Consignee', 'Container Number', 'HBL',\n",
    "        'Manufacturer', 'Steamship Line', 'Vessel', 'Voyage', 'Origin Port', 'Discharge Port',\n",
    "        'Final Discharge', 'Req. Ship Date', 'Cargo Ready Date', 'ETD', 'Actual Ship Date', 'Discharge Date',\n",
    "        'Loaded on Rail', 'ETA Ramp', 'ETA', 'Status', 'Hot Container', 'Last Free Date Pull',\n",
    "        'Last Free Date Return', 'Date Pulled', 'Date Returned', 'Freight Cost',\n",
    "        'Customs Clearance Date', 'BL Release Date', 'Customer Importer of Record', 'Door', 'Linked Orders',\n",
    "        'Container Notes', 'Scanner Notes', 'Receipt Date', 'Maersk DIR', 'External PO ID'])\n",
    "\n",
    "    check_list = [col for col in complete_df]\n",
    "\n",
    "    complete_df = complete_df[~(complete_df[check_list[5:]].isna().all(1))]\n",
    "    complete_df.replace({'mismatch': None})\n",
    "\n",
    "    print(complete_df.head(20).to_string())\n",
    "\n",
    "    returned_df = returned_df[['container_number', 'mbl_number', 'gnosis_scac', 'gnosis_mother_vessel',\n",
    "                               'gnosis_mother_voyage', 'gnosis_origin_port', 'gnosis_pod_city',\n",
    "                               'gnosis_final_discharge', 'gnosis_vessel_atd', 'gnosis_discharged_dt',\n",
    "                               'gnosis_rail_departed_dt', 'gnosis_ramp_eta', 'gnosis_eta', 'gnosis_demurrage_day',\n",
    "                               'gnosis_out_gate', 'gnosis_detention_day', 'gnosis_empty_returned',\n",
    "                               'gnosis_customs_clearance', 'gnosis_carrier_release', 'org_container_name', 'org_scac',\n",
    "                               'org_mother_vessel', 'org_mother_voyage', 'org_origin_port', 'org_pod_city',\n",
    "                               'org_final_discharge', 'org_vessel_atd', 'org_discharged_dt',\n",
    "                               'org_rail_departed_dt', 'org_ramp_eta', 'org_eta', 'org_demurrage_day',\n",
    "                               'org_out_gate', 'org_detention_day', 'org_empty_returned',\n",
    "                               'org_customs_clearance', 'org_carrier_release', 'org_status_id', 'notes', 'exception']]\n",
    "    # return upload_df, exceptions, returned_df\n",
    "    return complete_df, exceptions, returned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exporter(original_path, with_scac, without_scac, container_df, act_df,\n",
    "             upload_df, exception_df, returned_df, today_date):\n",
    "    os.chdir(original_path)\n",
    "    folder = r'\\\\fileserver\\public\\Purchasing\\Logistics\\Gnosis\\API'\n",
    "    os.chdir(folder)\n",
    "    file_name = 'API_Results_' + today_date.strftime(\"%Y_%m_%d\") + '.xlsx'\n",
    "\n",
    "    sheet = [{'Name': 'POST_Scac', 'df': with_scac, 'max_row': with_scac.shape[0],\n",
    "              'max_col': (with_scac.shape[1] - 1)},\n",
    "             {'Name': 'POST', 'df': without_scac, 'max_row': without_scac.shape[0],\n",
    "              'max_col': (without_scac.shape[1] - 1)},\n",
    "             {'Name': 'Containers', 'df': container_df, 'max_row': container_df.shape[0],\n",
    "              'max_col': (container_df.shape[1] - 1)},\n",
    "             {'Name': 'Active', 'df': act_df, 'max_row': act_df.shape[0],\n",
    "              'max_col': (act_df.shape[1] - 1)},\n",
    "             {'Name': 'Upload', 'df': upload_df, 'max_row': upload_df.shape[0],\n",
    "              'max_col': (upload_df.shape[1] - 1)},\n",
    "             {'Name': 'Exception', 'df': exception_df, 'max_row': exception_df.shape[0],\n",
    "              'max_col': (exception_df.shape[1] - 1)},\n",
    "             {'Name': 'Returned', 'df': returned_df, 'max_row': returned_df.shape[0],\n",
    "              'max_col': (returned_df.shape[1] - 1)},\n",
    "             ]\n",
    "\n",
    "    writer = pd.ExcelWriter(file_name, engine='xlsxwriter')\n",
    "\n",
    "    # iteration through sheets\n",
    "    for worksheets in sheet:\n",
    "        worksheets['df'].to_excel(writer, sheet_name=worksheets['Name'], startrow=1, header=False, index=False)\n",
    "        worksheet = writer.sheets[worksheets['Name']]\n",
    "        column_settings = []\n",
    "\n",
    "        # getting list of headers\n",
    "        for header in (worksheets['df']).columns:\n",
    "            column_settings.append({'header': header})\n",
    "\n",
    "        # setting table conditions\n",
    "        worksheet.add_table(0, 0, worksheets['max_row'], worksheets['max_col'],\n",
    "                            {'columns': column_settings,\n",
    "                             'style': 'Table Style Medium 2'})\n",
    "\n",
    "        # setting column widths\n",
    "        for column in worksheets['df']:\n",
    "            column_width = 20\n",
    "            col_idx = (worksheets['df']).columns.get_loc(column)\n",
    "            worksheet.set_column(col_idx, col_idx, column_width)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    os.chdir(r'\\\\fileserver\\public\\Purchasing\\Logistics\\Gnosis\\Upload')\n",
    "    upload_df.to_csv('Upload Templates-' + today_date.strftime('%#m.%#d') + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_scheduler_run_stamp(start_path):\n",
    "    upload_row = ['', '']\n",
    "    os.chdir(start_path)\n",
    "    os.chdir('../../../../../../Project Files/Function_Projects/Task_Error_Email/Data/Input')\n",
    "    folder = os.path.abspath(os.curdir)\n",
    "    with open(folder + '/Task_Scheduler_Log.csv', mode='a') as upload_file:\n",
    "        upload_writer = csv.writer(upload_file, delimiter=',', lineterminator='\\n',\n",
    "                                   quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        upload_row[0] = 'Gnosis_API_New'\n",
    "        upload_row[1] = (datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        upload_writer.writerow(upload_row)\n",
    "    upload_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.chdir('C:\\\\Repositories\\\\Analytics_Repository\\\\Function_Projects\\\\Gnosis_API\\\\Code\\\\Scripts')\n",
    "    original_path = os.getcwd()\n",
    "    today_date = date.today()\n",
    "    conn_read = connect_read()\n",
    "    container_df = get_post_data(conn_read, original_path)\n",
    "    auth_tkn = auth()\n",
    "    with_scac, without_scac = postings(container_df, auth_tkn)\n",
    "    container_df, act_df = containers(auth_tkn)\n",
    "    # with_scac, without_scac, container_df, act_df = readin(original_path, today_date)\n",
    "    checked_df = checker(container_df, conn_read)\n",
    "    upload_df = manipulations(act_df, checked_df, today_date)\n",
    "    complete_df, exception_df, returned_df = prep_upload(upload_df, today_date)\n",
    "    exporter(original_path, with_scac, without_scac, container_df, act_df,\n",
    "             complete_df, exception_df, returned_df, today_date)\n",
    "    task_scheduler_run_stamp(original_path)\n",
    "\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# INIT\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
